{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fab8d0a",
   "metadata": {},
   "source": [
    "# SGP 2025 - Deep Learning on Meshes and Point Clouds tutorial\n",
    "\n",
    "In this tutorial we give a quick introduction in the basics of deep learning on a point cloud. We will:\n",
    "1. Set up the Python environment with the right libraries.\n",
    "2. Load a dataset and visualize the contents.\n",
    "3. Set up a deep learning model on this dataset.\n",
    "4. Train the model and visualize its outputs.\n",
    "\n",
    "During the tutorial, you will find some lines marked with $\\rightarrow$.<br />\n",
    "$\\rightarrow$ Pause and try out the tasks get deeper understanding.\n",
    "\n",
    "## Step 1: Set up the Python environment\n",
    "In this Notebook, we'll need the following libraries:\n",
    "- PyTorch - a general library for deep learning, accelerated on GPU.\n",
    "- PyTorch Geometric (pyg) - an 'extension' library for learning on graphs and geometric data.\n",
    "- Tensorboard - a framework to monitor training and visualize outputs.\n",
    "- Polyscope - a convenient and extensible 3D viewer.\n",
    "- Meshplot - a 3D viewer that can run inside Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch_geometric as pyg\n",
    "    import polyscope as ps\n",
    "    import meshplot as mp\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    print(\"One of the dependencies is not installed. Installing now...\")\n",
    "    %pip install torch torchvision\n",
    "    %pip install torch_geometric torch_cluster polyscope tqdm\n",
    "    %pip install matplotlib pythreejs meshplot@git+https://github.com/skoch9/meshplot/@725e4a7926a5f10888f0edd1762fecf9db751c56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ec7f8",
   "metadata": {},
   "source": [
    "### Testing the environment\n",
    "We want to make sure that all the libraries we installed work (we expect no errors when running the imports) and the CUDA components are available, if you have a GPU.\n",
    "\n",
    "Do not worry if CUDA support says `False`, you can still run the tutorial on your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA support: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639dfcf",
   "metadata": {},
   "source": [
    "## Step 2: Load a dataset and visualize the contents.\n",
    "\n",
    "We will use PyTorch Geometric's (pyg) built-in dataset classes to load the ModelNet10 dataset from the [“3D ShapeNets: A Deep Representation for Volumetric Shapes”](https://people.csail.mit.edu/khosla/papers/cvpr2015_wu.pdf) paper. If you'd like to make your own datasets, be sure to check out the [PyTorch Geometric tutorial on \"Creating Graph Datasets\"](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html).\n",
    "\n",
    "We've created a smaller version of the dataset for this tutorial. Let's first download and unzip that, so PyTorch Geometric does not download the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6724443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url_modelnet10 = \"https://www.dropbox.com/scl/fi/x9pjg7orfv1f0gwcz3vux/ModelNet10.zip?rlkey=ji5zjyt52v8aipz0ecdhqqx7u&st=oem5rp7v&dl=1\"\n",
    "raw_dir = 'data/raw'\n",
    "if not os.path.exists(raw_dir):\n",
    "    os.makedirs(raw_dir)\n",
    "\n",
    "path = pyg.data.download_url(url_modelnet10, raw_dir)\n",
    "pyg.data.extract_zip(path, raw_dir)\n",
    "os.unlink(path)\n",
    "pyg.io.fs.rm(os.path.join(raw_dir, '__MACOSX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pyg.datasets.ModelNet(root='data', name='10', train=True, force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f598829",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset. The main thing you need to know for now is that you can access the elements in the dataset using Python's `[]` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List all class names in the ModelNet10 dataset\n",
    "class_names = [n for n in sorted(os.listdir(dataset.raw_dir)) if os.path.isdir(os.path.join(dataset.raw_dir, n))]\n",
    "\n",
    "# Access one object in the dataset\n",
    "# We retrieve a Data object, which contains the vertex positions, faces and a label.\n",
    "print(f\"Number of objects in the dataset: {len(dataset)}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Class names: {class_names}\\n\")\n",
    "\n",
    "print(f\"Data object 0: {dataset[0]}\")\n",
    "print(f\"Object 0 label: {class_names[dataset[0].y.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f23ca",
   "metadata": {},
   "source": [
    "It's good practice to visualize your data before and after processing and applying transformations (e.g., computing a neighborhood graph, normalizing scale). This lets you catch bugs or problems with the data early on.\n",
    "\n",
    "We'll use Polyscope one time and proceed with `meshplot` in the rest of the notebook. Polyscope is great for interactive inspection and it lets you prototype tools and complex visualizations.\n",
    "\n",
    "*Note 1: If you're on Mac, the Polyscope window may open in the background.*<br />\n",
    "*Note 2: Close the Polyscope window to stop the process running in this notebook.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35674ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.init()\n",
    "ps.register_surface_mesh(\"ModelNet Object\", \n",
    "                         # We need to convert from torch to numpy for polyscope\n",
    "                          dataset[0].pos.numpy(),\n",
    "                                   # We transpose the face array, because polyscope expects faces to be in [F, 3] format \n",
    "                          dataset[0].face.T.numpy())\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874f531",
   "metadata": {},
   "source": [
    "If you're working in the cloud on a Jupter notebook, it's easier to use `meshplot`, because it runs directly within Jupyter. We'll continue with `meshplot` in the remainder, so you can also follow along in a cloud hosted notebook.\n",
    "\n",
    "$\\rightarrow$ Try plotting a couple of different meshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "mp.plot(dataset[index].pos.numpy(), dataset[index].face.T.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0942708",
   "metadata": {},
   "source": [
    "It's good to know where data resides in PyTorch. Tensors (the core data objects in PyTorch, i.e., n-dimensional arrays) can be stored near the CPU (RAM) or on the GPU (VRAM). Whenever you want to perform an action with the CPU or GPU, you need to make sure that the tensor is placed on the right device. \n",
    "\n",
    "**However, beware of going back and forth between CPU and GPU!** One of the biggest bottlenecks in GPU computing is the trip down memory lane, the literal one. It is preferrable to move data only once (although it's definitely possible to move back-and-forth). For example, you can first process your data on the CPU and move it to the GPU in batches.\n",
    "\n",
    "Our data currently resides on the CPU, because we haven't told it to move to the GPU. If we want that to happen, we can use `tensor.to('cuda')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "pos = dataset[0].pos\n",
    "\n",
    "print(f\"Device for pos before moving to the GPU: {pos.device}\")\n",
    "pos = pos.to(device)\n",
    "print(f\"Device for pos after moving to the GPU: {pos.device} (remains 'cpu' if you do not have a GPU available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4bb8a",
   "metadata": {},
   "source": [
    "### Converting to a point cloud and then a graph\n",
    "In the lecture for this course, we discussed the graph-based approach to deep learning on meshes and point clouds. We can transform the meshes in this dataset to a point cloud by sampling points on the faces.\n",
    "\n",
    "$\\rightarrow$ What would be the issue with directly using the mesh vertices? How can we account for those issues?\n",
    "\n",
    "**Hint:** Look at a few of the meshes that we plotted before. What would happen if the triangles change, but the surface stays the same (e.g., you subdvide some triangles)?\n",
    "\n",
    "#### Transforms\n",
    "These processing steps are implemented in pyg as _transforms_. They are applied to all objects in the dataset and (a) stored on disk if you give it as a pre-transform or (b) applied on-the-go when loading the objects if given as a normal transform.\n",
    "\n",
    "We setup the following pre-transforms:\n",
    "- Normalize each mesh to a unit cube.\n",
    "- Sample points on the faces of the mesh (removes faces and original vertices).\n",
    "- Create a kNN graph for each point cloud.\n",
    "\n",
    "We apply these as pre-transforms, because we would like to re-use these steps every time we train.\n",
    "\n",
    "During training, we would like to apply a random rotation and scale, to make the method robust to these operations when testing. It's better if we do not use these as pre-transforms, because we want the rotation and scale to be different every time we load a shape. We add those as regular transforms, which are applied when an object is loaded during training.\n",
    "\n",
    "**Note:** processing can take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88837a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "# Transforms applied before loading the dataset to memory, \n",
    "# these are applied only once and not repeated during training\n",
    "pre_transform = T.Compose((\n",
    "    T.NormalizeScale(),\n",
    "    T.SamplePoints(1024), # If we want to sample more points, we can change this number\n",
    "    T.KNNGraph(k=20, loop=False)\n",
    "))\n",
    "\n",
    "# Transforms applied during loading, repeated during training\n",
    "# We can use these to augment the data\n",
    "# For example, we can randomly scale and rotate the objects\n",
    "transform = T.Compose((\n",
    "    T.RandomScale((0.85, 1.15)),\n",
    "    T.RandomRotate(45, axis=0),\n",
    "    T.RandomRotate(45, axis=1),\n",
    "    T.RandomRotate(45, axis=2))\n",
    ")\n",
    "\n",
    "# We load the dataset with the pre-transforms and the transform\n",
    "train_dataset = pyg.datasets.ModelNet(root='data', name='10', train=True, transform=transform, pre_transform=pre_transform, force_reload=True)\n",
    "# And also the test dataset. Note that we do not apply the data augmentation transforms to the test set\n",
    "test_dataset = pyg.datasets.ModelNet(root='data', name='10', train=False, transform=transform, pre_transform=pre_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fbd3bb",
   "metadata": {},
   "source": [
    "$\\rightarrow$ Again, visualize the point clouds for several shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "mp.plot(train_dataset[index].pos.numpy(), shading={\"point_size\": 0.1})\n",
    "\n",
    "# Alternatively, visualize the point cloud in polyscope\n",
    "# ps.init()\n",
    "# ps.register_point_cloud(\"ModelNet Object\",\n",
    "#                         train_dataset[index].pos.numpy(), radius=0.0002)\n",
    "# ps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018ba57",
   "metadata": {},
   "source": [
    "The dataset lets you load one `Data` object at a time. What if you want to send multiple shapes at the same time (e.g., for Stochastic Gradient Descent)? In that case, we need to put a batch of shapes together.\n",
    "\n",
    "This is handled by a `DataLoader`. The data loader also allows you to set a number of workers. You can think of this as the number of parallel processes to load and process your data from RAM. You can tweak this, so your CPU can 'catch up' with the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create data loaders for the training and test datasets\n",
    "# We let the train data loaders automatically batch the data and shuffle it for training with the `shuffle=True` argument\n",
    "train_loader = pyg.loader.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = pyg.loader.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c11b1",
   "metadata": {},
   "source": [
    "## Step 3: Set up a deep learning model on this dataset.\n",
    "\n",
    "We will now setup a simple PointNet++ model using Message Passing. This code is based on the [Point Cloud Processing tutorial in the PyTorch Geometric documentation.](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/point_cloud.html)\n",
    "\n",
    "PyTorch works with Modules. Modules are objects that process and output data, often with some parameters that are stored in the object. This allows PyTorch to keep track of the parameters that need to be optimized. For example, in the code below, we create a `PointNetLayer`, which is a descendant of `torch.nn.Module` (through `MessagePassing`). The MLP used in PointNet is initialized when the `PointNetLayer` is constructed, so that the weights in the MLP can be optimized.\n",
    "\n",
    "$\\rightarrow$ Follow the comments in the code to understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5695c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "\n",
    "class PointNetLayer(MessagePassing):\n",
    "    \"\"\"We implement a PointNet layer based on the MessagePassing class from PyTorch Geometric.\n",
    "    The MessagePassing class requires a function to compute the message\n",
    "    and a forward function, that you can fill in with your own logic.\n",
    "\n",
    "    Aggregation is handled by the MessagePassing class and only requires\n",
    "    that you set the `aggr` argument in the constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        # Message passing with \"max\" aggregation.\n",
    "        # TODO: Experiment with other aggregations like \"mean\" or \"add\".\n",
    "        super().__init__(aggr='max')\n",
    "\n",
    "        # Initialization of the MLP used to compute messages:\n",
    "        # Here, the number of input features correspond to the hidden\n",
    "        # node dimensionality plus point dimensionality (=3).\n",
    "        self.mlp = Sequential(\n",
    "            Linear(in_channels + 3, out_channels),\n",
    "            ReLU(),\n",
    "            Linear(out_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "        h: Tensor,\n",
    "        pos: Tensor,\n",
    "        edge_index: Tensor,\n",
    "    ) -> Tensor:\n",
    "        # Start propagating messages.\n",
    "        return self.propagate(edge_index, h=h, pos=pos)\n",
    "\n",
    "    def message(self,\n",
    "        h_i: Tensor,\n",
    "        h_j: Tensor,\n",
    "        pos_j: Tensor,\n",
    "        pos_i: Tensor,\n",
    "    ) -> Tensor:\n",
    "        # h_j: The features of the central node as shape [num_edges, in_channels]\n",
    "        # h_j: The features of neighbors as shape [num_edges, in_channels]\n",
    "        # pos_j: The position of neighbors as shape [num_edges, 3]\n",
    "        # pos_i: The central node position as shape [num_edges, 3]\n",
    "\n",
    "        # TODO: Experiment with different edge features (e.g., EdgeConv, GCN)\n",
    "        edge_feat = torch.cat([h_j, pos_j - pos_i], dim=-1)\n",
    "        return self.mlp(edge_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d67ff",
   "metadata": {},
   "source": [
    "Once the main building block (the PointNetLayer) is created, we can stack them together into a simple PointNet++ architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "\n",
    "class PointNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # We setup two PointNet layers and add a simple classifier at the end.\n",
    "        self.conv1 = PointNetLayer(3, 32)\n",
    "        self.conv2 = PointNetLayer(32, 32)\n",
    "        self.classifier = Linear(32, dataset.num_classes)\n",
    "\n",
    "    def forward(self,\n",
    "        pos: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        batch: Tensor,\n",
    "    ) -> Tensor:\n",
    "\n",
    "        # Perform two-layers of message passing:\n",
    "        h = self.conv1(h=pos, pos=pos, edge_index=edge_index)\n",
    "        h = h.relu()\n",
    "        h = self.conv2(h=h, pos=pos, edge_index=edge_index)\n",
    "        h = h.relu()\n",
    "\n",
    "        # Global Pooling:\n",
    "        h = global_max_pool(h, batch)  # [num_examples, hidden_channels]\n",
    "\n",
    "        # Classifier:\n",
    "        return self.classifier(h)\n",
    "\n",
    "\n",
    "model = PointNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250278e6",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "$\\rightarrow$ Can you change the PointNet++ module to GCN?<br/>\n",
    "$\\rightarrow$ And to EdgeConv?\n",
    "\n",
    "**Hint:** You only need to make minimal changes to the `message` function, the type of `aggr` in the `PointNetLayer`, and the input dimensions of the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca56f22",
   "metadata": {},
   "source": [
    "## Step 4: Train the model and visualize its outputs.\n",
    "\n",
    "Now that we have a model and dataloader, we can start optimizing the model. We do this with one of PyTorch's built in optimizers ([Adam](https://arxiv.org/pdf/1412.6980), a gradient descent optimizer using adaptive momentum).\n",
    "\n",
    "$\\rightarrow$ Follow the comments to understand the code.\n",
    "\n",
    "**Note:** The accuracy will remain quite low ($50-60\\%$). This is because the training set is much smaller than usual (only 20 shapes per class, rather than 450) and the model depth is limited. If you want to try a 'full' training loop, delete the `raw` folder in `data` and re-run the notebook, without downloading the smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = PointNet().to(device) # Move the model to the GPU if available\n",
    "# Create the optimizer.\n",
    "# We let the optimizer know which parameters to optimize and the learning rate (step size).\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# We create the loss function, here a Cross Entropy Loss, typically used for classification tasks.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the training loop\n",
    "def train():\n",
    "    # Set the model to training mode, which enables certain features like dropout, if they are used.\n",
    "    model.train()\n",
    "\n",
    "    # Initialize a total loss accumulator for reporting.\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        # First set all the gradients to zero, because we accumulate gradients by default in PyTorch.\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)  # Move the data to the GPU if available\n",
    "\n",
    "        # Forward pass: compute the logits (predictions) for the input data.\n",
    "        logits = model(data.pos, data.edge_index, data.batch)\n",
    "        # Compute the loss between the logits and the ground truth labels.\n",
    "        loss = criterion(logits, data.y)\n",
    "\n",
    "        # Backward pass: compute the gradients of the loss with respect to the model parameters.\n",
    "        loss.backward()\n",
    "        # Update the model parameters using the optimizer.\n",
    "        optimizer.step()\n",
    "        # Accumulate the loss for reporting.\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "# We do not want to compute gradients during testing, so we use torch.no_grad()\n",
    "# This saves memory and speeds up the computation.\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    # Set the model to evaluation mode, which disables features like dropout.\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)  # Move the data to the GPU if available\n",
    "        logits = model(data.pos, data.edge_index, data.batch)\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        total_correct += int((pred == data.y).sum())\n",
    "\n",
    "    return total_correct / len(test_loader.dataset)\n",
    "\n",
    "# Training loop for 50 epochs\n",
    "for epoch in tqdm((range(1, 51))):\n",
    "    loss = train()\n",
    "    test_acc = test()\n",
    "    tqdm.write(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6070322",
   "metadata": {},
   "source": [
    "### Visualizing the output\n",
    "To finish up, we show some models and their predicted classes in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61045ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "data = test_dataset[index]\n",
    "data = data.to(device)  # Move the data to the GPU if available\n",
    "logits = model(data.pos, data.edge_index, data.batch)\n",
    "pred = logits.argmax(dim=-1)\n",
    "print(f\"Predicted class: {class_names[pred.item()]}, True class: {class_names[data.y.item()]}\")\n",
    "mp.plot(data.pos.cpu().numpy(), shading={\"point_size\": 0.1})\n",
    "\n",
    "# Alternatively, visualize the point cloud in polyscope\n",
    "# ps.init()\n",
    "# ps.register_point_cloud(\"ModelNet Object\",\n",
    "#                         data.pos.numpy(), radius=0.0002)\n",
    "# ps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3470e1",
   "metadata": {},
   "source": [
    "## Further reading and resources\n",
    "Check out more tutorials on PyTorch Geometric in the documentation. For example, the tutorial on [Point Cloud Processing](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/point_cloud.html) is quite relevant.\n",
    "\n",
    "### Advanced tasks to try:\n",
    "$\\rightarrow$ Can you implement a self-attention mechanism within the `MessagePassing` paradigm? If you need inspiration, take a look at some of the convolution layers in PyTorch Geometric (e.g., Graph Attention), but you'll learn more if you first try it yourself.<br />\n",
    "$\\rightarrow$ If you have a mesh with varying triangle densities over the surface (this is the default case), you could weight messages passed from a vertex by the area around that vertex. In geometry processing, we use a mass matrix for this ([LibIGL has a function to compute this for a mesh](https://libigl.github.io/libigl-python-bindings/igl_docs/#massmatrix)). Can you include the mass matrix in the message passing algorithm and train on meshes instead of point clouds?\n",
    "\n",
    "**Note:** This is not guaranteed to improve or work better, but it's a step in the right direction. If your training data has a consistent connectivity (meshing), your method can overfit on the connectivity. This is highly problematic, because in most real-world applications, the connectivity is not guaranteed and your method will fail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgp_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
